%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                      Homework 1                            %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letter]{article}

\usepackage{lipsum}
\usepackage[pdftex]{graphicx}
\usepackage[margin=1.5in]{geometry}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{framed} 
\usepackage{amsmath}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{enumerate}

%%%%%%%%%%%%%%%
%% DOC INFO %%%
%%%%%%%%%%%%%%%
\newcommand{\bHWN}{2}
\newcommand{\bCLASS}{CS 189}

\title{\bCLASS: Homework \bHWN}
\author{William Guss\\26793499\\wguss@berkeley.edu}

\fancyhead[L]{\bCLASS}
\fancyhead[CO]{Homework \bHWN}
\fancyhead[CE]{GUSS}
\fancyhead[R]{\thepage}
\fancyfoot[LR]{}
\fancyfoot[C]{}
\usepackage{csquotes}




\pagestyle{fancy}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\newenvironment{menumerate}{%
  \edef\backupindent{\the\parindent}%
  \enumerate%
  \setlength{\parindent}{\backupindent}%
}{\endenumerate}

%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{menumerate}
\item Let $a = 1/\sqrt{3}, b = 1, c = \sqrt{3}.$ Then recall that 
\begin{equation}
  E[G] = 4\int_0^a f(x) dx + 3 \int_a^b f(x) dx + 2\int_b^c f(x) dx.
\end{equation}
Using babies first calculus class, we get that 
\begin{equation}
\int f(x) dx = \frac{\arctan(x)}{\pi},
\end{equation}
giving us  
\begin{equation}
E[G] = \frac{4}{3} + \frac{1}{3} + \frac{1}{3} = 2.
\end{equation}
\newpage
\item Maximum Likelihood Estimation?
Recall from wikipedia, that since $f(x;\theta)$ for each is generated independently
and identically distributed, we have
\begin{equation}
f(x_1,x_2\dots;\theta) = \prod_i^n \theta e^{-\theta x_n}.
\end{equation}
We want to find a value $\theta$ which maximizes the average log-likihood, given by
\begin{equation}
  \ell = \frac{1}{n} \sum \ln (\theta e^{-\theta x_i}) = \sum \ln (\theta) -\theta x_i.
\end{equation}
Using calculus, we look for $\theta$ satisfying
\begin{equation}
  \begin{aligned}
    \ell' = \sum \frac{1}{\theta} - x_i &= 0 \\
    \frac{n}{\theta}  &= \sum x_i\\
    \theta &= \frac{n}{\sum x_i}.
  \end{aligned}
\end{equation}
Applying the values we get $\sum x_i = 5.9$, $n = 5$, and $\theta = 0.847457627.$

\newpage
\item Let $A$ be a positive definite matrix in $\mathbb{R}^{n\times n}.$
  \begin{menumerate}
    \item Consider the following derrivation:
    \begin{equation}
      \begin{aligned}
          x^TAx = x^T \begin{bmatrix}
            \sum_j^n  a_{1j}x_j \\
            \vdots\\
            \sum_j^n a_{nj}x_j
          \end{bmatrix}  = \sum_i^n \sum_j^n a_{ij}x_ix_j.
       \end{aligned}   
    \end{equation}
    \item 
    \begin{theorem}
      If $A$ is positive definite, then the diagonals of $A$ are positive.
    \end{theorem}
    \begin{proof}
      Suppose that there is negative value on the diagonal, say $a_{qq}.$
      Then let $x = e_{q}.$ If we apply the quadratic form we get $e_qA^Te_q = a_qq < 0.$ 
      This contradicts the positive semidefiniteness of $A.$
    \end{proof}
  \end{menumerate}

\newpage

  \item Short Proofs. 
  \begin{menumerate}
    \item Assume problem (b).
    \begin{lemma}
      If $A$ is a matrix with eigen values $\lambda_n$  $A + \gamma I$ has eigenvalues $\gamma + \lambda_n$   
    \end{lemma}
    \begin{proof}
      If $\lambda_n$ is an eigenvalue, then $Av_n = \lambda_n v_n$  for a corresponding eigenvector $v.$
      Furthermore 
      \begin{equation}
       (A + \gamma I)v = Av + \gamma I v = \lambda_n v + \gamma v = (\lambda_n + \gamma)
       \end{equation} 
       which implies that $\lambda_n + \gamma$ is an eigen value of $A + \gamma I.$ This completes the proof.  
    \end{proof}
    \begin{theorem}
      If $A$ is positive semidefinite and $\gamma > 0,$ then $A + \gamma I$ is positive definite.
    \end{theorem}
    \begin{proof}
      If $A$ is positive definite then by the logic of the proof of (b), 
      \begin{equation}
          \begin{aligned}
            x^T A x = \sum_i \lambda_i (x_i^T e_i)^2 \geq 0.         
          \end{aligned}
         \end{equation}   
      It follows that some $\lambda \geq 0$ since $x \neq 0.$ Therefore by the previous lemma adding 
      $\gamma$ to the diagonal adds $\gamma$ to every eigenvalue implying that all eigen values are positive.
      By (b), $A + I\gamma$ is positive definiute therefore.
      \end{proof}
    \item Lolololol!
    \begin{theorem}
      $A$ is positive definite if and only if all of its eigen values are more than 0.
    \end{theorem}
    \begin{proof}
      Iff $A$ is positive semidefinite then it is symmetric. Using spectral theorem we have that 
      \begin{equation}
        \begin{aligned}
          x^TAx = \sum_i (x^Te_i)e_i^TAx &= \sum_i = x^Te_ie_i^T\lambda e_i^Tx \\
              &= \sum_i \lambda_i (x^Te_i)^2 > 0
         \end{aligned}   
      \end{equation}   
      which is true if and only if all $\lambda_i$ are more than 0.
    \end{proof}
    \item 
    \begin{theorem}
      If $A$ is positive definite then it is invertible.   
    \end{theorem}
    \begin{proof}
      The invertible matrix theorem statesa that a matrix is invertible if and only if all of its eigen values
      are more than $0$. By the previous theorem if $A$ is positive definite then all of its eigen values
       are positive and so it is invertible.   
    \end{proof}
    \item
    \begin{theorem}
      If $A$ is positive definite then there exist $n$ linearly independent vectors so that 
      $A_{ij} = x^T_ix_j.$   
    \end{theorem}
    \begin{proof}
      The statement of the theorem is true if and only if $A = B^TB$ where $B$ is invertible. 
      By spectral theorem we have that $A = U\Lambda U^T$ where $\Lambda = diag(\lambda_1, \dots, \lambda_n).$
      Furthermore $U^{-1} = U^T$. Let $\Omega = diag(\sqrt \lambda_1, \dots, \sqrt \lambda_n).$ Then, 
      $\Omega^2 = \Lambda.$ Let $W^T = U\Omega$ and $W = \Omega U^T.$ So we have that $W$ is still an orthonormal matrix
      and so $A = W^TW.$ This completes the proof.
    \end{proof}
  \end{menumerate}


  \newpage
  \item DERIVATIONS :( Assuming theorems from Math 105
  \begin{menumerate}
      \item Consider the followiong derivation
      \begin{equation}
        \begin{aligned}
          \frac{\partial(x^Ta)}{\partial x} = \frac{\partial (x)}{\partial x}^T a + \left(\frac{\partial (a)}{\partial x}\right)^T x
          &= a.
        \end{aligned}
       \end{equation}
       \item Consider the following derivation
       \begin{equation}
          \begin{aligned}
              \frac{\partial(x^TAx)}{\partial x} = \frac{\partial(x^T)}{\partial x} Ax + \frac{\partial(Ax)}{\partial x}^TX = Ax + Ax^T
          \end{aligned}
       \end{equation}
       \item Consider the following derivation
       \item
       \begin{theorem}
          If $x \in \mathbb{R}^n$ 
          \begin{equation}
            \|x\|_2 \leq \|x\|_1 \leq \sqrt{n}\|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}.
          \end{equation}
       \end{theorem}
       \begin{proof}
            Squaring the first two terms of the inequality shows that
            $\|x\|_2^2$ has fewer terms than $\|x\|_1.$ 

            Now define the following vector, $e$, so that $e_i = 1$ if
            $x_i$ is positive and $e_i = -1$ if $x$ is negative.
            Then $\langle x, e \rangle = \sum_i |x_i| = \|x\|_1.$

            Cauchy schwartz says that $\langle x, e \rangle \leq \|x\|\|e\| = \|x\|_2\sqrt{n}.$
            This completes the proof.
       \end{proof}
  \end{menumerate}
  \newpage
  \item Weighted Linear Lolzs.
  \begin{menumerate}
    \item Consider the following.
    \begin{equation}
        \begin{aligned}
          R[w] = \sum_i \lambda_i (w^T x_i - y_i)^2 &=\sum_i (w^T x_i - y_i) \lambda_i (w^T x_i - y_i)  \\
          &=\sum_i v^T \Lambda v. 
        \end{aligned}
    \end{equation}
    Observe that $v = (w^Tx_i - y_i, \dots)^T = (w^Tx_i,dots) - Y = Xw - Y.$ Therefore 
    \begin{equation}
      R[w] = (Xw - Y)^T\Lambda (Xw - Y).
    \end{equation}
    \item We can use the linearity of matrix multiplication to derive the following expression:
    \begin{equation}
      \begin{aligned}
        \frac{\partial R}{\partial w} &= \frac{\partial}{\partial w} (Xw)^T\Lambda (Xw - Y) - Y^T\Lambda (Xw - Y) \\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y - Y^T\Lambda (Xw) + Y^T \Lambda Y \\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y- \frac{\partial}{\partial w}  Y^T\Lambda Xw \\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y - (Y^T\Lambda X) \\
         &= X^T \Lambda Xw + (\Lambda X)^T Xw - X^T \Lambda Y  - Y^T\Lambda X = 0. \\
      \end{aligned}
    \end{equation}
    And so we can manipulate the expression so that
    \begin{equation}
      \begin{aligned}
          &(X^T \Lambda X + (\Lambda X)^T X)w  =  X^T \Lambda Y  + Y^T\Lambda X \\
          w  &=  ((X^T \Lambda X + (\Lambda X)^T X)^{-1}(X^T \Lambda Y  + Y^T\Lambda X) \\
          &=  X^{-1}(X^T \Lambda  + X^T\Lambda^T )^{-1}(X^T \Lambda Y  + Y^T\Lambda X) \\
          &=  2X^{-1}(X^T\Lambda)^{-1}(X^T \Lambda Y  + Y^T\Lambda X) \\
          &=  2(X^T\Lambda X)^{-1}(X^T \Lambda Y  + Y^T\Lambda X) \\
      \end{aligned}
    \end{equation}
    \item Adding $L_2$ regularization! Gives us
    \begin{equation}
      R[w] = (Xw - Y)^T\Lambda (Xw - Y) +  w^T\gamma I w.
    \end{equation}
    Taking dthe derivative we get
    \begin{equation}
      \begin{aligned}
        \frac{\partial R}{\partial w} &= \frac{\partial}{\partial w} (Xw)^T\Lambda (Xw - Y) - Y^T\Lambda (Xw - Y) +   \frac{\partial }{\partial w}w^T\gamma I w\\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y - Y^T\Lambda (Xw) + Y^T \Lambda Y  + \frac{\partial }{\partial w}w^T\gamma I w \\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y- \frac{\partial}{\partial w}  Y^T\Lambda Xw  +\frac{\partial }{\partial w}w^T\gamma I w\\
         &= \frac{\partial}{\partial w} (Xw)^T\Lambda Xw - (Xw)^T\Lambda Y - (Y^T\Lambda X)  + \frac{\partial }{\partial w}w^T\gamma I w\\
         &= X^T \Lambda Xw + (\Lambda X)^T Xw + 2\gamma I w - X^T \Lambda Y  - Y^T\Lambda X = 0. \\
      \end{aligned}
    \end{equation}
    And so we can manipulate the expression so that
    
    \begin{equation}
      \begin{aligned}
          &(X^T \Lambda X + (\Lambda X)^T X + 2I\gamma)w  =  X^T \Lambda Y  + Y^T\Lambda X \\
          w  &=  ((X^T \Lambda X + (\Lambda X)^T X + 2I\gamma)^{-1}(X^T \Lambda Y  + Y^T\Lambda X) \\
          &=  \frac12(X^T\Lambda X + I\gamma)^{-1}(X^T \Lambda Y  + Y^T\Lambda X) .\\
      \end{aligned}
    \end{equation}
    Essentially we add to the least squares pseudo inverse $\gamma I,$ thereby increasing its eigen values.
    This may allow us to find a solution when $X^T\Lambda X$ is non-singular. ie. $L_2$ regulartization penalizes
    movement in any (infinite) direction too far away from a small solution, it also forces a solution with $\gamma$ large enough.
  \end{menumerate}
  \newpage
  \item Doubt Classes!
  \begin{menumerate}
    \item We wish to minimize risk with respect to $i$. So, 
    observe the logic of the policy. If the probability that $\omega_j$ is the output
    given $x$ is less than that with respect to $i$ then we wish to eliminate this large contribution to the some.
    It must furthermore be that such a probability be at least less than the loss incurred
    by the doubt. That is consider the expected value $l(...) = \lambda_s$ in the case that we don't 
    choose the doubt. So then $\lambda_s(1-\lambda_r/\lambda_s) = \lambda_s - \lambda_r$ if
    and only if the doubt in this situation has less 'weight' than making the prediction. 
    Therefore this policy makes sense.
    \item In the case that there is no loss incurred by doubting,
    it follows that using the minimum risk strategy immediately implies
    that fior every training example we choose to doubt unless
    $P(\omega_i|x) = 1$, in which case we are $100\%$ certain
    that our classification estimate is correct. This agrees with my intuition.

    In the case thgat $\lambda_r > \lambda_s$, the intuition is that choosing to
    doubt our predicition is more "negativeley" impactful then to choose our prediction
    itsself; that is, more loss is incurred if we tend towards a doubt class.
    Therefore by our "minimum" risk procedure, we should choose to accept the prediction
    instead of the doubt every single time. ( $P(\omega_i|x) \geq 0 > 1 - m $ where $m > 1$.)
  \end{menumerate}
  \newpage
  \item Gaussians
  \begin{menumerate}
    \item We want to equate the two distributions so:
    \begin{equation}
      \begin{aligned}
               P(\omega_1 | x) &= \frac12 P(x | \omega_1) = \frac{1}{2\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_1)^2}{2\sigma^2}\right) \\
                &= \frac12 P(x | \omega_2) = \frac{1}{2\sqrt{2\pi\sigma^2}}exp\left(\frac{(x-\mu_2)^2}{2\sigma^2}\right) \\
                \implies  &(x-\mu_1)^2 = (x-\mu_2)^2 \\
      \end{aligned}
     \end{equation} 
     So we get the plus or minus definition of $\mu_1.$ This completes the derivation.
  \end{menumerate}
\end{menumerate}

\end{document}





