{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 Random Forests and Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a Decision tree using Shannon Entropy and expected information gain.\n",
    "To do this we need to\n",
    "* Implement functions getting information entropy\n",
    "* Implement a decision tree class which acts on a \"labeled\" dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on the standard definition of entropy.\n",
    "def entropy(data, classes):\n",
    "    entr = 0\n",
    "    for cls in classes:\n",
    "        probi = len(cls)/len(data)\n",
    "        entr += -probi*math.log2(probi)\n",
    "    \n",
    "    return entr\n",
    "\n",
    "# Determines the information gain for an attribute for a data set.\n",
    "def discrete_igain(data, classes, a, possible_a):\n",
    "    data_entropy = entropy(data,classes)\n",
    "    return discrete_igain_fast(data,classes,a,possible_a,data_entropy)\n",
    "\n",
    "def discrete_igain_fast(data,classes, a , possible_a, data_entropy):\n",
    "    mutualinf = 0\n",
    "    for attr in possible_a:\n",
    "        data_attr = [x for x in data if x[a] == attr]\n",
    "        mutualinf += len(data_attr)/len(data)*entropy(data_attr, classes)\n",
    "    \n",
    "    return  data_entropy - mutualinf\n",
    "\n",
    "#not an efficient classes algorithm, but no fucks given; only use once.\n",
    "def get_classes(data, labels):\n",
    "    cls = []\n",
    "    label_vals = np.unique(labels)\n",
    "    for label in label_vals:\n",
    "        cls.append([x for x,y in zip(data,labels) if y == label])\n",
    "        \n",
    "    return cls\n",
    "\n",
    "def get_attrvals(data, a):\n",
    "    return np.unique(np.transpose(data)[a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XOR!\n",
    "x = np.array([[0,0],\n",
    "              [1,0],\n",
    "              [0,1],\n",
    "              [1,1],\n",
    "              [-1,1],\n",
    "              [-1,0],\n",
    "              [-1,-1]])\n",
    "y = np.array([0,1,1,0,0,0,1])\n",
    "\n",
    "cls = get_classes(x,y)\n",
    "entropy(x, cls)\n",
    "discrete_igain(x, cls, 1, get_attrvals(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def submap(data,label, restriction):\n",
    "    indices, constraints = zip(*restriction)\n",
    "    \n",
    "    a,b = zip(*[(x,y) for x,y in zip(data,label) if x[indices] == constraints])\n",
    "    return (np.array(list(a)), np.array(list(b)))\n",
    "xp, yp =submap(x,y, [(0,-1)])\n",
    "clsp = get_classes(xp,yp)\n",
    "\n",
    "def tabstr(n):\n",
    "    retstr = \"\"\n",
    "    for i in range(n):\n",
    "        retstr += \"\\t\"\n",
    "        \n",
    "    return retstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discrete Decision Trees\n",
    "Since we've built our tests and informtion theoretic methods; it is now probably a good idea to build a decision tree class. This should attempt to maximize information gain upon construction and thereafter be able to classify\n",
    "any given training example confined to the attributional classes of the dataset.\n",
    "\n",
    "After we do this, we'll define a decision tree which can take discrete and continuous vales, specified during construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A toy example of the discrete decision tree\n",
    "class ddtree:\n",
    "    ### gets decisional\n",
    "    def __init__(self, data, labels, restricted=[]):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.classes = get_classes(data,labels)\n",
    "        self.subtrees = {}\n",
    "        self.entropy = entropy(self.data, self.classes)\n",
    "        self.attr = None\n",
    "        self.restricted = restricted\n",
    "    \n",
    "    #trains the tree:\n",
    "    def train(self):\n",
    "        #check to see that the label set is unique\n",
    "        if len(np.unique(self.labels)) <= 1:\n",
    "            return\n",
    "        \n",
    "        #Get the igains for all of the attributes.\n",
    "        igains = []\n",
    "        \n",
    "        for i in range(self.data.shape[1]): #random forest would take a random sub sample.\n",
    "            #make sure we don't consider all the values a previous node has considered.\n",
    "            if self.restricted is None or i not in self.restricted:\n",
    "                atr_vals = get_attrvals(self.data,i)\n",
    "                \n",
    "                if len(atr_vals) >1: #We only want to consider attributes whose possible values are different\n",
    "                    igains.append(discrete_igain(self.data, self.classes, i, atr_vals))\n",
    "                \n",
    "        #Best attribute is the argmax\n",
    "        best_attr = np.argmax(igains)\n",
    "        self.attr = best_attr\n",
    "        \n",
    "        # restrict the attributes which the trees can consider!\n",
    "        subres = [best_attr]\n",
    "        subres.extend(self.restricted)\n",
    "        \n",
    "        #Make the sub decision trees for each choice of the attribute.\n",
    "        for val in get_attrvals(self.data,best_attr):\n",
    "            #make a subtree\n",
    "            dp, lp = submap(self.data, self.labels, [(best_attr, val)])\n",
    "            #make the subtree decisions based on if they satisfy a lambda; to abstract in dynamic trees.\n",
    "            tree = ddtree(dp, lp, subres)\n",
    "            self.subtrees[(val, lambda x, best_attr=best_attr, val=val: x[best_attr] == val)] = tree\n",
    "\n",
    "            \n",
    "        #train all of the trees\n",
    "        for (val, test), tree in self.subtrees.items():\n",
    "            tree.train()\n",
    "            \n",
    "    \n",
    "    def classify(self, x):\n",
    "        if len(self.subtrees) > 0 and self.attr is not None:\n",
    "            for (val, test), tree in self.subtrees.items():\n",
    "                if test(x):\n",
    "                    return tree.classify(x)\n",
    "        else:\n",
    "            return self.labels[0] #Return the only label in the tree.\n",
    "        \n",
    "    def print_tree(self, n):\n",
    "        if len(self.subtrees) > 0 and self.attr is not None:\n",
    "            retstr =  \"x[\" + str(self.attr) + \"] subtrees(\" + str(len(self.subtrees)) + \")\\n\" + tabstr(n)\n",
    "            for (val, test), tree in self.subtrees.items():\n",
    "                retstr += \"- \" +  str(val) + \" ->\"+\"\\t\" + tree.print_tree(n+1) +\"\\n\" + tabstr(n)\n",
    "        else:\n",
    "            return  \"Y:\" + str(self.labels[0])\n",
    "        \n",
    "        return retstr\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.print_tree(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = ddtree(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try training this thing!\n",
    "test.train()\n",
    "print(test)\n",
    "print(\"Classification error!\")\n",
    "for i in range(len(x)):\n",
    "    print(test.classify(x[i]), y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full decision trees with continuous values and random subset selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the full version of decsion trees, we don't think of attributes as elements of a set but in fact indicator functions of subsets. In the case of the `ddtree` we have $\\chi_a$ for singleton sets $\\{a\\} := a$. For the full thing we'll just extend this notion to \"axis aligned\" intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def indicator\n",
    "class Indicator:\n",
    "    def __init__(self, func, name):\n",
    "        self.func = func\n",
    "        self.name = name\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "    def __str__(self):\n",
    "        return str(self.name)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "\n",
    "# make our indicator functions.\n",
    "def dindicator(a):\n",
    "    return Indicator((lambda x,a=a: x == a),  str(a))\n",
    "def ci_indicator(a,b):\n",
    "    return Indicator((lambda x,a=a,b=b: a <= x and x < b), \"[\" +str(a) + \", \" + str(b) + \")\")  \n",
    "def cray_indicator(a, inf):\n",
    "    if inf > 0:\n",
    "        return Indicator((lambda x,a=a: a <= x), \"(\" + str(a) +\", inf)\")\n",
    "    elif inf < 0:\n",
    "        return Indicator((lambda x,a=a: x < a),  \"(-inf,\" + str(a) +\")\")\n",
    "    else:\n",
    "        return dindicator(a)\n",
    "    \n",
    "    \n",
    "def split_indicators(splits):\n",
    "    indicators = []\n",
    "    \n",
    "    #make end rays\n",
    "    indicators.append(cray_indicator(splits[0], -1))\n",
    "    \n",
    "    #make middle intervals\n",
    "    for i, val in enumerate(splits[:-1]):\n",
    "        indicators.append(ci_indicator(splits[i], splits[i+1]))\n",
    "    #make end rays\n",
    "    indicators.append(cray_indicator(splits[-1], 1))\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "def singleton_indicator(possible_vals):\n",
    "    indicators = []\n",
    "    for val in possible_vals:\n",
    "        indicators.append(dindicator(val))\n",
    "    \n",
    "    return indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to now make the indicator versions of our helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Based on the standard definition of entropy.\n",
    "def entropy(data, classes):\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    entr = 0\n",
    "    for cls in classes:\n",
    "        cls = np.intersect1d(cls, data)\n",
    "        probi = len(cls)/len(data)\n",
    "        entr += -probi*math.log2(probi)\n",
    "    \n",
    "    return entr\n",
    "\n",
    "# Determines the information gain for an attribute for a data set.\n",
    "def igain(data, classes, attribute, indicators):\n",
    "    data_entropy = entropy(data,classes)\n",
    "    return igain_fast(data, classes, attribute, indicators, data_entropy)\n",
    "\n",
    "def igain_fast(data, classes, attribute, indicators, data_entropy):\n",
    "    mutualinf = 0\n",
    "    for indicator in indicators:\n",
    "        data_attr = [x for x in data if indicator(x[attribute])]\n",
    "        mutualinf += len(data_attr)/len(data)*entropy(data_attr, classes)\n",
    "    \n",
    "    return  data_entropy - mutualinf\n",
    "\n",
    "#not an efficient classes algorithm, but no fucks given; only use once.\n",
    "def get_classes(data, labels):\n",
    "    cls = []\n",
    "    label_vals = np.unique(labels)\n",
    "    for label in label_vals:\n",
    "        cls.append([x for x,y in zip(data,labels) if y == label])\n",
    "        \n",
    "    return cls\n",
    "\n",
    "def get_indicators(data, classes, a, discrete=True, heuristic=None):\n",
    "    if discrete:\n",
    "        return singleton_indicator(np.unique(np.transpose(data)[a]))\n",
    "    else:\n",
    "        if heuristic is None:\n",
    "            return split_indicators([np.mean(np.transpose(data)[a])])\n",
    "        else:\n",
    "            return heuristic(data, classes, a, np.transpose(data)[a])\n",
    "\n",
    "# Submaps data based on restrictions.\n",
    "def submap(data,label, restriction):\n",
    "    def satisfies(x, restriction=restriction):\n",
    "        for index, constraint in restriction:\n",
    "            if not constraint(x[index]):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    valueset = [(x,y) for x,y in zip(data,label) if satisfies(x)]\n",
    "    if not valueset:\n",
    "        return None, None\n",
    "    else:\n",
    "        a,b = zip(*valueset)\n",
    "        return (np.array(list(a)), np.array(list(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitpoint Heuristics\n",
    "We need heuristics for data deemed continuous. How should we split it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#Breaks an interval up into n random splits\n",
    "def get_random_splits(n, M, m):\n",
    "        splits = []\n",
    "        for i in range(n):\n",
    "            splits.append(random.random()*(M-m) + m)\n",
    "        splits.sort()\n",
    "        return splits\n",
    "\n",
    "def random_heuristic(n=None):\n",
    "    def get_splits(data, classes, a, values, n=n):\n",
    "        if n is None:\n",
    "            n = math.floor(math.sqrt(len(values)))\n",
    "        M,m = np.max(values), np.min(values)\n",
    "        \n",
    "        splits = get_random_splits(n, M, m)\n",
    "        return split_indicators(splits)\n",
    "    \n",
    "    return get_splits\n",
    "\n",
    "def entropy_heuristic():\n",
    "    def get_splits(data, classes, a, values):\n",
    "        possible_indicators = []\n",
    "        values = np.unique(values)\n",
    "        for value in values:\n",
    "            possible_indicators.append(split_indicators([value]))\n",
    "            \n",
    "        \n",
    "        igains = list(map(lambda x: igain(data, classes, a, x), possible_indicators))\n",
    "        \n",
    "        return possible_indicators[np.argmax(igains)]\n",
    "    \n",
    "    return get_splits\n",
    "\n",
    "# Gets the most adventageous information gain according to splits\n",
    "# TODO TEST:\n",
    "def random_entropy_heuristic(n=None,p=100):\n",
    "    def get_splits(data,classes, a, values, n=n, p=p):\n",
    "        \n",
    "        possible_indicators = []\n",
    "        rando_splitter = random_heuristic(n)\n",
    "        for i in range(p):\n",
    "            possible_indicators.append(rando_splitter(data, classes,a, values))\n",
    "        \n",
    "        igains = map(lambda x: igain(data, classes, a, x), possible_indicators)\n",
    "         \n",
    "        \n",
    "        return possible_indicators[np.argmax(igains)]\n",
    "    \n",
    "    return get_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "Now that we've resolved the problem to acting on arbitrary indicator functions for subsets, we can consider building a general decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A toy example of the discrete decision tree\n",
    "class dtree:\n",
    "    ### gets decisional\n",
    "    def __init__(self, data, labels, restricted=[], depth=0, maxdepth=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.classes = get_classes(data,labels)\n",
    "        self.subtrees = {}\n",
    "        self.entropy = entropy(self.data, self.classes)\n",
    "        self.attr = None\n",
    "        self.depth = depth\n",
    "        self.restricted = restricted\n",
    "        self.maxdepth = maxdepth\n",
    "\n",
    "    def bag(self, attributes, bag):\n",
    "        if bag > 0:\n",
    "            return random.sample(attributes, min(bag, len(attributes)))\n",
    "        else:\n",
    "            return attributes\n",
    "                                \n",
    "                             \n",
    "    #trains the tree:\n",
    "    def train(self, bag=0, typetable=None, heuristic=None):\n",
    "        #check to see that the label set is unique\n",
    "        if len(np.unique(self.labels)) <= 1:\n",
    "            return\n",
    "        \n",
    "        #Get the igains for all of the attributes.\n",
    "        igains = []\n",
    "        attr_indic = {}\n",
    "        \n",
    "        attributes = [ i for i in range(len(self.data[0])) if i not in self.restricted]\n",
    "        attributes = self.bag( attributes, bag)\n",
    "        \n",
    "        \n",
    "        if not attributes or self.depth == self.maxdepth:\n",
    "            return #We've reached the end of our tree.\n",
    "        \n",
    "        for i in attributes: #random forest would take a random sub sample.\n",
    "            #make sure we don't consider all the values a previous node has considered.\n",
    "            if self.restricted is None or i not in self.restricted:\n",
    "                discrete = False\n",
    "                #Assume that all data is discrete :()\n",
    "                if typetable is False:\n",
    "                    discrete = False              \n",
    "                elif typetable is None or typetable[i]:\n",
    "                    discrete = True\n",
    "                \n",
    "                indicators = get_indicators(self.data, self.classes, i, discrete, heuristic)\n",
    "                # In case this is the best indicator    \n",
    "                attr_indic[i] = indicators\n",
    "                \n",
    "                #We only want to consider attributes whose possible values are different\n",
    "                igains.append(igain(self.data, self.classes, i, indicators))\n",
    "        \n",
    "        \n",
    "        #Best attribute is the argmax\n",
    "        best_attr = attributes[np.argmax(np.array(igains))]\n",
    "        self.attr = best_attr\n",
    "        \n",
    "        # restrict the attributes which the trees can consider!\n",
    "        subres = [best_attr]\n",
    "        subres.extend(self.restricted)\n",
    "        \n",
    "        \n",
    "        #Make the sub decision trees for each choice of the attribute.\n",
    "        for val in attr_indic[best_attr]:\n",
    "            #make a subtree\n",
    "            dp, lp = submap(self.data, self.labels, [(best_attr, val)])\n",
    "            \n",
    "            #Only make a subtree if there are datapoints which would even satisfy it!\n",
    "            if dp is not None:\n",
    "                #make the subtree decisions based on if they satisfy a lambda; to abstract in dynamic trees.\n",
    "                tree = dtree(dp, lp, subres, depth=self.depth+1, maxdepth=self.maxdepth)\n",
    "                self.subtrees[(val, lambda x, best_attr=best_attr, val=val: val(x[best_attr]))] = tree\n",
    "\n",
    "            \n",
    "        #train all of the trees\n",
    "        for (val, test), tree in self.subtrees.items():\n",
    "            tree.train(bag, typetable, heuristic)\n",
    "            \n",
    "    \n",
    "    def classify(self, x):\n",
    "        if len(self.subtrees) > 0 and self.attr is not None:\n",
    "            for (val, test), tree in self.subtrees.items():\n",
    "                if test(x):\n",
    "                    return tree.classify(x)\n",
    "        else:\n",
    "            return stats.mode(self.labels)[0][0] #Return the only label in the tree.\n",
    "        \n",
    "    def print_tree(self, n):\n",
    "        if len(self.subtrees) > 0 and self.attr is not None:\n",
    "            retstr =  \"x[\" + str(self.attr) + \"] subtrees(\" + str(len(self.subtrees)) + \")\\n\" + tabstr(n)\n",
    "            for (val, test), tree in self.subtrees.items():\n",
    "                retstr += \"- \" + str(val) + \" ->\"+\"\\t\" + tree.print_tree(n+1) +\"\\n\" + tabstr(n)\n",
    "        else:\n",
    "            return  \"Y:\" + str(self.labels[0])\n",
    "        \n",
    "        return retstr\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.print_tree(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow that was a lot. Okay, so if that worked; we should definitely be able to the discrete case! :e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dtree(x,y) #Shit it worked!\n",
    "test.train()\n",
    "print(test)\n",
    "print(\"Classification error!\")\n",
    "for i in range(len(x)):\n",
    "    print(test.classify(x[i]), y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shit that worked. Let's try some continuous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#XOR!\n",
    "x = np.array([[0,0],\n",
    "              [0,0.1],\n",
    "              [0,0.2],\n",
    "              [0,0.3],\n",
    "              [0,0.4],\n",
    "              [0,0.5],\n",
    "              [0,0.6],\n",
    "              [0,0.7],\n",
    "              [0,0.8],\n",
    "              [0,0.9],\n",
    "              [0,1.0],])\n",
    "y = np.array([0,0,0,0,0,1,1,1,1,1,1])\n",
    "\n",
    "cls = get_classes(x,y)\n",
    "typetable = {}\n",
    "typetable[0] = True\n",
    "typetable[1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = dtree(x,y)\n",
    "test.train(typetable=typetable, heuristic=entropy_heuristic())\n",
    "print(test)\n",
    "print(\"Classification error!\")\n",
    "for i in range(len(x)):\n",
    "    print(test.classify(x[i]), y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "Now we can aggregate our forest fun with random forests who decorolate decision trees and take therir average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not meant for regression!\n",
    "class rforest:\n",
    "    #initalizes all the trees and partitions the data.\n",
    "    def __init__(self, data, labels, numtrees, partition_size, maxdepth=None):\n",
    "        self.trees = []\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.partitions = []\n",
    "        \n",
    "        #create partitions\n",
    "        for i in range(numtrees):\n",
    "            \n",
    "            dpart, lpart = zip(*random.sample(list(zip(data,labels)), partition_size))\n",
    "           \n",
    "            self.partitions.append((dpart, lpart))\n",
    "            self.trees.append(dtree(np.array(list(dpart)), np.array(list(lpart)), maxdepth=maxdepth))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self, bag=0, typetable=None, heuristic=None):\n",
    "        for tree in self.trees:\n",
    "            tree.train(bag, typetable, heuristic)\n",
    "        \n",
    "    \n",
    "    #Ensemble voting\n",
    "    def ensemble(self,x):\n",
    "        box = {}\n",
    "        for tree in self.trees:\n",
    "            vote = tree.classify(x)\n",
    "            if vote is not None:\n",
    "                if vote in box:\n",
    "                    box[vote] += 1\n",
    "                else:\n",
    "                    box[vote] = 1\n",
    "        \n",
    "        \n",
    "        return box\n",
    "    \n",
    "    def classify(self, x):\n",
    "        predictions = self.ensemble(x)\n",
    "        i, m = None, -1\n",
    "        for pred, votes in predictions.items():\n",
    "            if votes > m:\n",
    "                i,m = pred, votes\n",
    "        \n",
    "        return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now let's try this thing on that x,y dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = rforest(x,y, 6, 3)\n",
    "test.train(heuristic=entropy_heuristic(), bag=1)\n",
    "print(\"Classification error!\")\n",
    "for i in range(len(x)):\n",
    "    print(test.ensemble(x[i]), y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do Some fucking Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_data = scipy.io.loadmat(\"../data/spam_dataset/spam_data.mat\")\n",
    "spam_train = spam_data['training_data']\n",
    "spam_test = spam_data['test_data']\n",
    "\n",
    "spam_train_data_raw =  spam_train\n",
    "spam_train_label_raw =  spam_data['training_labels']\n",
    "\n",
    "spam_tlabel = spam_train_label_raw.ravel()\n",
    "div_train = [1.0/max(arr) for arr in spam_train_data_raw.T]\n",
    "spam_tdata = spam_train_data_raw \n",
    "spam_test_data =  spam_test\n",
    "\n",
    "#Shuffle that spam data good.\n",
    "shuffle = np.random.permutation(np.arange(spam_tdata.shape[0]))\n",
    "spam_tdata, spam_tlabel = spam_tdata[shuffle], spam_tlabel[shuffle]\n",
    "\n",
    "#VALIDATION\n",
    "spam_valid_data  =  spam_tdata[0:750]\n",
    "spam_valid_label = spam_tlabel[0:750]\n",
    "\n",
    "#TRAINING\n",
    "spam_train_data  =spam_tdata[750:]\n",
    "spam_train_label =spam_tlabel[750:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ents = rforest(spam_train_data, spam_train_label, 20, int(len(spam_train_data)/5), maxdepth=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ents.train(typetable=False, heuristic=random_entropy_heuristic(3, 100),bag=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Classification error!\")\n",
    "net_error = 0\n",
    "for i in range(len(spam_valid_data)):\n",
    "    pred = ents.classify(spam_valid_data[i]) \n",
    "    if pred is None:\n",
    "        print(pred)\n",
    "    if pred is not None:\n",
    "        net_error += abs(pred - spam_valid_label[i])\n",
    "    \n",
    "print(net_error/len(spam_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_results = []\n",
    "\n",
    "\n",
    "for i, dp in enumerate(spam_test_data):\n",
    "    pred = ents.classify(dp) \n",
    "    if pred is None:\n",
    "        pred = 0\n",
    "    spam_results.append(np.array([i+1, pred]))\n",
    "\n",
    "np.savetxt(\n",
    "    'kagglespam.csv',           # file name\n",
    "    spam_results,                # array to save\n",
    "    fmt='%i',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "raw_census_data = []\n",
    "numerical = ['age', 'fnlwgt','education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "with open('../data/census_data/train_data.csv') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    \n",
    "    for row in reader:\n",
    "        raw_census_data.append(row)\n",
    "        \n",
    "\n",
    "census_data = []\n",
    "census_label =[]\n",
    "seen = {}\n",
    "\n",
    "for data in raw_census_data:\n",
    "    dp = []\n",
    "    for attr in data:\n",
    "        if attr == \"label\":\n",
    "            census_label.append(int(data[attr]))\n",
    "            \n",
    "        if data[attr] == '?':\n",
    "            dp.append(0)\n",
    "            \n",
    "        if attr in numerical:\n",
    "            dp.append(int(data[attr]))\n",
    "        else:\n",
    "            if attr not in seen:\n",
    "                seen[attr] = []\n",
    "            \n",
    "            if data[attr] not in seen[attr]:\n",
    "                seen[attr].append(data[attr])\n",
    "            dp.append(seen[attr].index(data[attr]))\n",
    "    \n",
    "    census_data.append(dp)\n",
    "    \n",
    "typetable = [x == 0 for x in census_data[0]]\n",
    "    \n",
    "census_train_data = census_data    \n",
    "census_label= np.array(census_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "raw_census_data = []\n",
    "numerical = ['age', 'fnlwgt','education-num', 'capital-gain', 'capital-loss', 'hours-per-week' ]\n",
    "with open('../data/census_data/test_data.csv') as csvFile:\n",
    "    reader = csv.DictReader(csvFile)\n",
    "    \n",
    "    for row in reader:\n",
    "        raw_census_data.append(row)\n",
    "        \n",
    "\n",
    "census_data = []\n",
    "\n",
    "for data in raw_census_data:\n",
    "    dp = []\n",
    "    for attr in data:\n",
    "        if attr == \"label\":\n",
    "            census_label.extend(int(data[attr]))\n",
    "            \n",
    "        if data[attr] == '?':\n",
    "            dp.append(0)\n",
    "            \n",
    "        if attr in numerical:\n",
    "            dp.append(int(data[attr]))\n",
    "        else:\n",
    "            if attr not in seen:\n",
    "                seen[attr] = []\n",
    "            \n",
    "            if data[attr] not in seen[attr]:\n",
    "                seen[attr].append(data[attr])\n",
    "            dp.append(seen[attr].index(data[attr]))\n",
    "    \n",
    "    census_data.append(dp)\n",
    "    \n",
    "\n",
    "census_test_data = census_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ents = rforest(census_train_data, census_label, 20, int(len(census_train_data)/5), maxdepth=20)\n",
    "len(typetable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ents.train(typetable=typetable, heuristic=entropy_heuristic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Classification error!\")\n",
    "net_error = 0\n",
    "for i in range(len(census_train_data)):\n",
    "    pred = ents.classify(census_train_data[i]) \n",
    "    if pred is None:\n",
    "        print(pred)\n",
    "    if pred is not None:\n",
    "        net_error += abs(pred - census_label[i])\n",
    "    \n",
    "print(net_error/len(spam_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_results = []\n",
    "\n",
    "\n",
    "for i, dp in enumerate(census_test_data):\n",
    "    pred = ents.classify(dp) \n",
    "    if pred is None:\n",
    "        pred = 0\n",
    "    spam_results.append(np.array([i+1, pred]))\n",
    "\n",
    "np.savetxt(\n",
    "    'census.csv',           # file name\n",
    "    spam_results,                # array to save\n",
    "    fmt='%i',             # formatting, 2 digits in this case\n",
    "    delimiter=',',          # column delimiter\n",
    "    newline='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
